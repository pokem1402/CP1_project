{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "VGeo4-bNdhaZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22MjIpysaX6t",
        "outputId": "05295ef0-f431-440f-d430-348e60aa033e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "# 1. 데이터를 불러오는 함수\n",
        "def load_data(src : str) -> List[pd.DataFrame]:\n",
        "\n",
        "  data : pd.DataFrame = pd.read_csv(src)\n",
        "  x : pd.DataFrame = data.iloc[:, :-1]\n",
        "  y : pd.DataFrame = data.iloc[:, -1]\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "v8RB9J_ueBZ9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "# 2. 가중치와 편향을 초기화하는 함수\n",
        "# 초기화에 관한 부분에 대한 reference : https://reniew.github.io/13/\n",
        "def initialize_parameters(x : np.ndarray or pd.DataFrame, d_out:int,\n",
        "                          init:str = \"xavier_normal\",\n",
        "                          use_bias:bool=True) -> Union[np.array, List[np.array]]:\n",
        "\n",
        "    d_in : int = x.shape[1] # 입력 변수의 차원\n",
        "\n",
        "    # 편향을 사용하는 경우\n",
        "    if use_bias:\n",
        "      # 초기화를 수월하게 진행할 수 있도록\n",
        "      # 가중치의 크기에 편향의 크기를 넣어서 계산한다.\n",
        "      w_size : tuple = (d_in+1, d_out)\n",
        "    # 편향을 사용하지 않는 경우\n",
        "    else:\n",
        "      w_size : tuple = (d_in, d_out)\n",
        "\n",
        "    # he 초기화의 경우\n",
        "    if init == \"he\":\n",
        "      # 분산의 값이 (2/입력)의 root 값\n",
        "      v : float = (2/d_in)**0.5\n",
        "      w : np.array = np.random.normal(0, v, w_size)\n",
        "    # Xavier 초기화 중 정규 분포를 통한 초기화인 경우\n",
        "    elif init == \"xavier_normal\":\n",
        "      v : float = (2/(d_in+d_out))**0.5\n",
        "      w : np.array = np.random.normal(0, v, w_size)\n",
        "    # Xavier 초기화 중 균일 분포를 통한 초기화인 경우\n",
        "    elif init == \"xavier_uniform\":\n",
        "      u : float = (6/(d_in+d_out))**0.5\n",
        "      w : np.array = np.random.uniform(-u, u, w_size)\n",
        "    else:\n",
        "      raise ValueError(f\"There is no implementation for [{init}] initialization\")\n",
        "\n",
        "    # 편향을 사용하는 경우\n",
        "    if use_bias:\n",
        "      # 가중치와 편향을 반환\n",
        "      return w[:-1], w[-1]\n",
        "    # 편향을 사용하지 않는 경우\n",
        "    else:\n",
        "      # 가중치만 반환\n",
        "      return w, np.zeros(d_out)"
      ],
      "metadata": {
        "id": "Yl_EmG9ud6-q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 데이터의 row를 셔플하는 함수\n",
        "def data_shuffle(x: pd.DataFrame, y: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "  idx = np.random.permutation(x.index)\n",
        "  x = x.reindex(idx)\n",
        "  y = y.reindex(idx)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "myb9ri2csQm8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "# 4. 학습 데이터와 테스트 데이터를 분리하는 함수\n",
        "def train_test_split(x : pd.DataFrame, y:pd.DataFrame,\n",
        "                     test_size : float = 0.2) -> List[pd.DataFrame]:\n",
        "  \n",
        "  # 데이터의 row 수\n",
        "  n : int = x.shape[0]\n",
        "\n",
        "  test_x : pd.DataFrame = x.iloc[:int(n*test_size)]\n",
        "  test_y : pd.DataFrame = y.iloc[:int(n*test_size)]\n",
        "  train_x : pd.DataFrame = x.iloc[int(n*test_size):]\n",
        "  train_y : pd.DataFrame = y.iloc[int(n*test_size):]\n",
        "  return train_x, train_y, test_x, test_y"
      ],
      "metadata": {
        "id": "lAebcZ0Zs7FE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterator\n",
        "# 5. mini-batch 생성기\n",
        "def mini_batch_generator(x : pd.DataFrame, y: pd.DataFrame,\n",
        "                         batch_size : int = 4) -> Iterator[pd.DataFrame]:\n",
        "  \n",
        "  n : int = x.shape[0]\n",
        "\n",
        "  x, y = data_shuffle(x, y)\n",
        "\n",
        "  for j in range(n//batch_size): # 데이터 크기를 배치 사이즈로 나눈만큼 반복한다.\n",
        "    yield x.iloc[j*batch_size:j*batch_size+batch_size], \\\n",
        "           y.iloc[j*batch_size:j*batch_size+batch_size]"
      ],
      "metadata": {
        "id": "nwSNlWKRyORo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. sigmoid 함수\n",
        "def sigmoid(x : np.ndarray or pd.DataFrame):\n",
        "  return 1./(1.+np.exp(-x))\n",
        "\n",
        "# 7. Binary Cross entropy\n",
        "def binaryCrossEntropy(y_true : np.ndarray or pd.DataFrame,\n",
        "                       y_pred : np.ndarray or pd.DataFrame,\n",
        "                       c : float = 1e-6) -> float:\n",
        "\n",
        "  y_true = np.squeeze(y_true)\n",
        "  y_pred = np.squeeze(y_pred)\n",
        "  # y의 실제값과 y의 예측값의 shape가 같지 않다면 ValueError\n",
        "  if y_true.shape != y_pred.shape:\n",
        "    raise ValueError(\"Shape of true value of y and prediction of y must be equal.\")\n",
        "\n",
        "  return - np.mean(y_true * np.log(y_pred+c))\n",
        "\n",
        "# 8. 정확도 연산 기능\n",
        "def get_accuracy(y_true : np.ndarray or pd.DataFrame,\n",
        "                 y_pred : np.ndarray or pd.DataFrame):\n",
        "  \n",
        "  y_true = np.squeeze(y_true)\n",
        "  y_pred = np.squeeze(y_pred)\n",
        "  # y의 실제값과 y의 예측값의 shape가 같지 않다면 ValueError\n",
        "  if y_true.shape != y_pred.shape:\n",
        "    raise ValueError(\"Shape of true value of y and prediction of y must be equal.\")\n",
        "\n",
        "  return np.mean(y_pred == y_true)\n",
        "\n"
      ],
      "metadata": {
        "id": "s0MQhsel2MbP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. 훈련 데이터를 통해 정확도를 계산하는 함수\n",
        "def compute_network_train_acc(x : np.ndarray or pd.DataFrame,\n",
        "                              y : np.ndarray or pd.DataFrame,\n",
        "                              w : np.ndarray, b : np.ndarray,\n",
        "                              batch_size : int = 4) -> float:\n",
        "    acc = []\n",
        "    # generator로부터 x와 y 값을 받아오는 generator\n",
        "    for batch_x, batch_y in mini_batch_generator(x, y, batch_size=batch_size):\n",
        "      # 행렬 연산\n",
        "      logit = sigmoid(np.matmul(batch_x, w)+ b)\n",
        "      # logit 값이 0.5보다 크다면 class 1 아니라면 0\n",
        "      y_pred = (logit >= 0.5).applymap(int)\n",
        "      acc.append(get_accuracy(batch_y, y_pred))\n",
        "    return np.mean(acc)"
      ],
      "metadata": {
        "id": "lN3BDO4zeU6C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시그모이드를 미분한 함수\n",
        "def sigmoid_derivative(x: np.ndarray or pd.DataFrame) -> Union[np.ndarray, pd.DataFrame]:\n",
        "\n",
        "  s = sigmoid(x)\n",
        "  return s * (1-s) \n",
        "\n",
        "# 역전파를 수행하는 함수\n",
        "# 역전파를 위한 미분 계산 : https://smwgood.tistory.com/6\n",
        "def backprop(x :np.ndarray or pd.DataFrame,\n",
        "             y : np.ndarray or pd.DataFrame,\n",
        "             logit : np.ndarray or pd.DataFrame,\n",
        "             w : np.ndarray, b : np.ndarray,\n",
        "             learning_rate :float = 1e-4) -> None:\n",
        "\n",
        "  # pandas 형식을 numpy array로 형식을 바꿔주고 차원 축을 동일하게 고정해둔다.\n",
        "  x = np.asarray(x)\n",
        "  y = np.asarray(np.squeeze(y))[:, np.newaxis]\n",
        "  logit = np.asarray(np.squeeze(logit))[:, np.newaxis]\n",
        "\n",
        "  # sigmoid를 activation으로 사용하는 cross entropy의 미분 값 계산\n",
        "  target_error : np.ndarray = y - logit\n",
        "\n",
        "  # 이전 항으로 미분 값 이전을 위한 체인 룰 계산\n",
        "  target_error_derivative : np.ndarray = target_error * sigmoid_derivative(logit)\n",
        "\n",
        "  # 가중치 업데이트\n",
        "  w_delta = learning_rate * np.mean(x.T.dot(target_error_derivative), axis=1)\n",
        "  w += w_delta[:, np.newaxis]\n",
        "\n",
        "  if len(b.shape) == 1:\n",
        "    b_delta = learning_rate * np.mean(target_error_derivative, axis=0)\n",
        "  else:\n",
        "    b_delta = learning_rate * np.mean(target_error_derivative, axis=1)[:, np.newaxis]\n",
        "\n",
        "  # 편향 업데이트\n",
        "  b += b_delta\n"
      ],
      "metadata": {
        "id": "4R9vXZ5Vd8c2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. 훈련 데이터를 통해 신경망 연산을 수행하는 함수\n",
        "def compute_network_train(x : np.ndarray or pd.DataFrame,\n",
        "                          y : np.ndarray or pd.DataFrame,\n",
        "                          w : np.ndarray, b : np.ndarray,\n",
        "                          batch_size : int = 4, epochs : int = 1) -> None:\n",
        "\n",
        "  # 손실값을 연산하는 함수\n",
        "  for epoch in range(epochs):\n",
        "    errors : list = []\n",
        "    for batch_x, batch_y in mini_batch_generator(x, y, batch_size = batch_size):\n",
        "      logit = sigmoid(np.matmul(batch_x, w) + b)\n",
        "      error = binaryCrossEntropy(batch_y, logit)\n",
        "      backprop(batch_x, batch_y, logit, w, b) # 역전파\n",
        "      errors.append(error)\n",
        "    errors = np.mean(errors)\n",
        "\n",
        "    acc = compute_network_train_acc(x,y,w,b,batch_size)\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] TrainData - Loss = {np.round(errors, 4)}, Accuracy = {np.round(acc,4)}\")    "
      ],
      "metadata": {
        "id": "ZcCMwd9YHPzF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. 테스트 데이터를 통해 정확도를 연산하는 함수\n",
        "def compute_network_test(x : np.ndarray or pd.DataFrame,\n",
        "                         y : np.ndarray or pd.DataFrame,\n",
        "                         w : np.ndarray, b : np.ndarray):\n",
        "\n",
        "  logit = sigmoid(np.matmul(x, w)+ b)\n",
        "  y_pred = (logit >= 0.5).applymap(int)\n",
        "  return get_accuracy(y, y_pred)"
      ],
      "metadata": {
        "id": "TRW11TlBbspJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main() -> None:\n",
        "  # file 위치\n",
        "  DATA_FILE_PATH = \"/content/drive/MyDrive/codestates/cp1/binary_dataset.csv\"\n",
        "\n",
        "  # (1) file load\n",
        "  x, y = load_data(DATA_FILE_PATH)\n",
        "  # (2) 파라미터와 편향 생성\n",
        "  w, b = initialize_parameters(x, 1, init=\"xavier_normal\")\n",
        "  # (3) 데이터 셔플링\n",
        "  x, y = data_shuffle(x,y)\n",
        "  # (4) 훈련 데이터와 테스트 데이터 분리\n",
        "  train_x, train_y, test_x, test_y = train_test_split(x, y)\n",
        "  # (5) 훈련 데이터를 통한 손실값 및 정확도 계산\n",
        "  compute_network_train(train_x, train_y, w, b, epochs=10)\n",
        "  # (6) 테스트 데이터 정확도 연산\n",
        "  test_acc = compute_network_test(test_x, test_y, w, b)\n",
        "  print(f\"TestData - Accuracy = {np.round(test_acc,4)}\") "
      ],
      "metadata": {
        "id": "7xNBvlHmW8QC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsygYLUoif1q",
        "outputId": "0de1e843-0636-4a9a-c697-1683c38fb59f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] TrainData - Loss = 4.3108, Accuracy = 0.6875\n",
            "[Epoch 2] TrainData - Loss = 4.2989, Accuracy = 0.6875\n",
            "[Epoch 3] TrainData - Loss = 4.1862, Accuracy = 0.6875\n",
            "[Epoch 4] TrainData - Loss = 4.1419, Accuracy = 0.6875\n",
            "[Epoch 5] TrainData - Loss = 4.026, Accuracy = 0.6875\n",
            "[Epoch 6] TrainData - Loss = 3.9074, Accuracy = 0.6875\n",
            "[Epoch 7] TrainData - Loss = 3.7885, Accuracy = 0.6875\n",
            "[Epoch 8] TrainData - Loss = 3.6416, Accuracy = 0.6875\n",
            "[Epoch 9] TrainData - Loss = 3.5642, Accuracy = 0.75\n",
            "[Epoch 10] TrainData - Loss = 3.4868, Accuracy = 0.75\n",
            "TestData - Accuracy = 0.75\n"
          ]
        }
      ]
    }
  ]
}